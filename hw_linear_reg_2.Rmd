---
title: "Homework 02"
author: "Xinyi Wang"
date: "Septemeber 16, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
library(dplyr)
library(ggplot2)
library(magick)
library(knitr)
opts_chunk$set(echo = TRUE)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
a = select(heights,earn,sex,height,yearbn,ed)
#Exclude NA
new_heights = na.omit(a)
#Exclude zero earings
earn_zero = which(new_heights$earn==0)
new_heights = new_heights[-earn_zero,]
#Assume male = 1, female = 2, factorise 'sex' variable
new_heights$sex = factor(new_heights$sex, labels=c("male", "female"))
#new_heights
summary(new_heights)
```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?

```{r}
h = new_heights$height
z.height = (h - mean(h)) / (2*sd(h))
height.lm = lm(earn ~ z.height,new_heights)
#display(height.lm)
summary(height.lm)
```

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
earn = new_heights$earn
height = new_heights$height
sex = new_heights$sex
ed = new_heights$ed
log.earn = log(new_heights$earn)

model1.lm = lm(earn ~ height + sex)
summary(model1.lm)
par(mfrow=c(2,2))
plot(model1.lm)
```
```{r}
model2.lm = lm(log.earn ~ height + sex)
summary(model2.lm)
par(mfrow=c(2,2))
plot(model2.lm)
```
```{r}
model3.lm = lm(log.earn ~ height + sex + ed + sex*ed)
summary(model3.lm)
par(mfrow=c(2,2))
plot(model3.lm)
```

Overall, model 3 is the preferred model since it has the largest r^2, residuals looks more likely equally spread around a horizontal line.

4. Interpret all model coefficients.

From model 3 we get, 

log(earn) = 8.106 + 0.0085*height - 0.984*sex + 0.097*ed + 0.038*sex*ed

The intercept 8.106 is the average log earning for a male has 0 height and 0 rate of education.

The coefficient for height is the predicted difference in log earning corresponding to male with every 1 inch difference in height.

The coefficient for sex is the predicted difference in log earning bewteen male and female if height and education rate are both 0.

The coefficient for education is the predicted difference in log earning corresponding to male with every 1 unit change in education rate.

The coefficient for sex:ed = 0.038 means 1 unit of education rate corresponds to 3.8% more of an increase in earnings among female than male.

5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
confint(model3.lm)
```

[6.84,9.37] is the range of values that you can be 95% certain contains the true value of intercept.

[-0.0089,0.0259] is the range of values that you can be 95% certain contains the true value coefficient of height.

[-1.536,-0.431] is the range of values that you can be 95% certain contains the true value coefficient of sex.

[0.0679,0.126] is the range of values that you can be 95% certain contains the true value coefficient of education.

[-0.00144,0.0773] is the range of values that you can be 95% certain contains the true value coefficient of sex:ed.

### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
nox = pollution$nox
mort = pollution$mort
plot(nox,mort)
model1.lm = lm(mort ~ nox)
plot(model1.lm,which = 1)
```

The model doesn't fit well. Residual plot also shows residuals are not symmetrically distributed around 0.

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
log.nox = log(nox)
model2.lm = lm(mort ~ log.nox)
ggplot(data = pollution, mapping = aes(x=log.nox,y=mort)) +
  geom_smooth(se = FALSE)+ 
  geom_point()
plot(model2.lm,which=1)
```

In model 2, we use log transformation in nox and the residual plot looks evenly distributed around 0, which is better than model 1.

3. Interpret the slope coefficient from the model you chose in 2.

```{r}
summary(model2.lm)
```

For each increase of 10^x in NOX, there is respective change of 15.335x in mortality.

4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(model2.lm,level=0.99)
```

[-2.230,32.901] is the range of values that you can be 99% certain contains the true value coefficient of log(nox).

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
so2 = pollution$so2
hc = pollution$hc
log.so2 = log(so2)
log.hc = log(hc)
model3.lm = lm(mort ~ log.nox + log.so2 + log.hc)
ggplot(data = pollution, mapping = aes(x=log.nox + log.so2 + log.hc,y=mort)) +
  geom_smooth(se = FALSE,method="lm",col="red")+ 
  geom_point()
summary(model3.lm)
```

The intercept means when nox, so2, hc level is 1, the average mortality rate is 924.965.

For each increase of 10^x in nox, so2, hc there is respective change of 58.336x, 11.762x, -57.3x in mortality rate.

6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
# split dataset into training and test sets
train = pollution[1:(nrow(pollution)/2), ]
test = pollution[((nrow(pollution)/2)+1):nrow(pollution), ]

# fit linear model
log.nox = log(train$nox)
log.so2 = log(train$so2)
log.hc = log(train$hc)
train.lm = lm(mort ~ log.nox + log.so2 + log.hc, data=train)
display(train.lm)

# Predict test half
prediction = predict(train.lm, test)
prediction
```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}
status = teengamb$status
z.status = (status - mean(status)) / (2*sd(status))
model1.lm = lm(gamble ~ sex + z.status + income + verbal, data = teengamb)
par(mfrow=c(2,2))
plot(model1.lm)
summary(model1.lm)
```

The intercept means average spends on gambling for male with same status, income, and verbal is 24.918.

The coefficient of sex means the difference in predicted expenditure on gambling between male (sex=0) and female (sex=1) will be -22.118.

For someone has same income and verbal level, with every 1 unit increase in status, spends in gambling will increase 1.803.

For someone has same status and verbal level, with every 1 unit increase in income will spends 4.962 more in gambling.

For someone has same status and income level, with every 1 unit increase in verbal level will spends 2.595 less in gambling.

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
confint(model1.lm)
```

[-6.392,56.229] is the range of values that you can be 95% certain contains the true value of intercept.

[-38.689,-5.547] is the range of values that you can be 95% certain contains the true value coefficient of sex.

[-17.78,21.39] is the range of values that you can be 95% certain contains the true value coefficient of mean of status.

[2.89,7.03] is the range of values that you can be 95% certain contains the true value coefficient of income.

[-7.34,1.42] is the range of values that you can be 95% certain contains the true value coefficient of verbal.

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
income = teengamb$income
verbal = teengamb$verbal
avg.data = data.frame(
  sex=0,z.status=mean(z.status),income=mean(income),verbal=mean(verbal))

avg.prediction = predict(model1.lm,avg.data,interval="confidence", level=0.95)
avg.prediction

max.data = data.frame(
  sex=0,z.status=max(z.status),income=max(income),verbal=max(verbal))
max.prediction = predict(model1.lm,max.data,interval="confidence", level=0.95)
max.prediction
```

Because the standard deviation of max.data is bigger than avg.data.

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
total = sat$total
log.total = log(sat$total)
expend = sat$expend
salary = sat$salary
ratio = sat$ratio
z.expend = (expend - mean(expend)) / (sd(expend))
z.salary = (salary - mean(salary)) / (sd(salary))
z.ratio = (ratio - mean(ratio)) / (sd(ratio))
model1.lm = lm(log.total ~ z.expend + z.ratio + z.salary,sat)
par(mfrow=c(2,2))
plot(model1.lm)
summary(model1.lm)
```

The intercept is the average total sat score when z.expend,z.ratio,and z.salary are 0 which is e^6.87 = 962.95.

The coefficient of z.expend means the difference in log sat score corresponding to a 1 standard-deviation difference in expend is 0.023.

The coefficient of z.ratio means the difference in log sat score corresponding to a 1 standard-deviation difference in ratio is 0.015.

The coefficient of z.salary means the difference in log sat score corresponding to a 1 standard-deviation difference in salary is -0.054.

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
confint(model1.lm,level=0.98)
```

[6.84,6.89] is the range of values that you can be 98% certain contains the true value of intercept.

[-0.05,0.09] is the range of values that you can be 98% certain contains the true value coefficient of z.expend.

[-0.021,0.05] is the range of values that you can be 98% certain contains the true value coefficient of z.ratio.

[-0.123,0.014] is the range of values that you can be 98% certain contains the true value coefficient of z.salary.

3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
takers = sat$takers
z.takers = (takers - mean(takers)) / (sd(takers))
model2.lm = lm(log.total ~ z.expend + z.ratio + z.salary + z.takers,sat)
summary(model2.lm)
plot(model1.lm,which=1)
plot(model2.lm,which=1)
```

The r^2 of model 2 is 0.83 which is much better than model 1. So the model with takers as predictor explain the outcome better.

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$

Advantage: Sems good transformation because is symmetric and centered at 0.

Disadvantage: Not proportional.This could limit the effectiveness of the predictor if districts differ widely in average money raised

* The ratio, $D_i/R_i$

Advantage: It is easy to interpret the effect of ratio.

Disadvantage: This transformation has the disadvantage of being centered at 1 and that is asymmetric. In particualar it tends to zero for case where the Republics have more money raised than Democrats, and tend to infinity on the opposite case.

* The difference on the logarithmic scale, $log D_i-log R_i$ 

Advantage:It is centered to zero and is symmetric; proportional to the magnitude of the difference.

Disadvantage: Similar to first transformation, not proportional.

* The relative proportion, $D_i/(D_i+R_i)$.

Advantage: This transformation is centered at 0.5 and symmetric.

Disadvantage: Similar to transformation 2, only indicate the effect of the relative ratio.

### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.


5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?


### Transformation 1-6 please see attached pdf 
		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

